## Inference of DL Models on Edge Devices viz NVIDIA Jetson Nano, AGX Orin.
This repo contains model compression(using TensorRT) and documentation of running various deep learning models on NVIDIA Jetson Orin, Nano (aarch64 architectures)
- [Pixelformer installation in Orin](./Pixelformer_installation_documentation/).
- [Model compression of Pixelformer](./MDE_demo_with_model_compression_code/).
- [SLAM on a Wearable Edge Device](./digital_heritage_project_demo_SLAM/).
